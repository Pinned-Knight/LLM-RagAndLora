{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e4b9b6",
   "metadata": {},
   "source": [
    "# Fine-tune Google FLAN-T5 Small with LoRA\n",
    "\n",
    "This notebook logs into Hugging Face, loads the `google/flan-t5-small` model for basic inference, and then fine-tunes it on a local JSON file of questionâ€“answer pairs using LoRA via the PEFT library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U --quiet transformers datasets peft accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf241702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# Log into Hugging Face (follow the popup in the notebook)\n",
    "# notebook_login()\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e8289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = 'google/flan-t5-small'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print('Model and tokenizer loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d100b4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roger Federer year end world number one\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Roger Federer year end world number one'\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb48c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'When was Roger Federer born?', 'answer': 'August 8, 1981'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Assumes `qa_data.json` exists in the same directory and contains a list of {\"question\": ..., \"answer\": ...}\n",
    "dataset = load_dataset('json', data_files='Federer_dataset.json', split='train')\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56a34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    input_text = example['question']\n",
    "    target_text = example['answer']\n",
    "    model_inputs = tokenizer(input_text, truncation=True, padding='max_length', max_length=128)\n",
    "    labels = tokenizer(target_text, truncation=True, padding='max_length', max_length=128)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "processed_dataset = dataset.map(preprocess, remove_columns=dataset.column_names, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d452e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,752,512 || all params: 79,713,664 || trainable%: 3.4530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q', 'v'],\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.generation_config = gen_config\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8759c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>43.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>43.285700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=43.348565673828126, metrics={'train_runtime': 68.0455, 'train_samples_per_second': 2.351, 'train_steps_per_second': 0.294, 'total_flos': 7773848862720.0, 'train_loss': 43.348565673828126, 'epoch': 10.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='flan-t5-small-lora',\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',\n",
    "    fp16=False,\n",
    "    label_names=[\"labels\"], \n",
    ")\n",
    "\n",
    "training_args.generation_config = gen_config\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5228d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter saved to flan_t5_small_lora_adapter\n"
     ]
    }
   ],
   "source": [
    "peft_model.save_pretrained('flan_t5_small_lora_adapter')\n",
    "print('LoRA adapter saved to flan_t5_small_lora_adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab7b49ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1897\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "lora_model = PeftModel.from_pretrained(base_model, 'flan_t5_small_lora_adapter')\n",
    "\n",
    "input_text = \"When was the Open Era started?\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "outputs = lora_model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "# api = HfApi()\n",
    "# api.upload_folder(\n",
    "#     repo_id='username/flan-t5-small-lora-adapter',\n",
    "#     folder_path='flan_t5_small_lora_adapter',\n",
    "#     repo_type='model',\n",
    "#     commit_message='Add LoRA adapter'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01688663",
   "metadata": {},
   "source": [
    "## FULL FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27cccf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 76,961,152 / 76,961,152 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "# Load base model & tokenizer\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load generation config\n",
    "gen_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config = gen_config\n",
    "\n",
    "# Make sure all weights are trainable\n",
    "model.train()\n",
    "\n",
    "# Print total trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable_params:,} / {total_params:,} \"\n",
    "      f\"({trainable_params / total_params:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51bd227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanay\\AppData\\Local\\Temp\\ipykernel_14460\\2673230133.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 15:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>31.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.161100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Tanay\\Desktop\\Projects\\Project 1\\project1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=2.07377796350047, metrics={'train_runtime': 918.5218, 'train_samples_per_second': 0.348, 'train_steps_per_second': 0.044, 'total_flos': 14871240376320.0, 'train_loss': 2.07377796350047, 'epoch': 20.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"flan_t5_small_finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=20,              # Adjust as needed\n",
    "    learning_rate=1e-2,\n",
    "    logging_steps=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False,                       # Set True if GPU supports\n",
    "    label_names=[\"labels\"],           # Needed for Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "training_args.generation_config = gen_config\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                      # ðŸ”¥ Use full model (no adapters)\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d5a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Full fine-tuned model saved to 'flan_t5_small_finetuned'\n"
     ]
    }
   ],
   "source": [
    "# Save fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"flan_t5_small_finetuned\")\n",
    "tokenizer.save_pretrained(\"flan_t5_small_finetuned\")\n",
    "\n",
    "print(\"ðŸŽ‰ Full fine-tuned model saved to 'flan_t5_small_finetuned'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54e345ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 times\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the fine-tuned model\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"flan_t5_small_finetuned\")\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(\"flan_t5_small_finetuned\")\n",
    "\n",
    "# Example inference\n",
    "input_text = \"Bruh\"\n",
    "inputs = finetuned_tokenizer(input_text, return_tensors='pt')\n",
    "outputs = finetuned_model.generate(**inputs, max_new_tokens=50)\n",
    "print(finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
